{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readArff(filename):\n",
    "    with open ('./UCI-Data/'+filename+'.arff', 'r') as f:\n",
    "        # split lines, remove ones with comments\n",
    "        lines = [line.lower() for line in f.read().split('\\n') if not line.startswith('%')]\n",
    "        \n",
    "    # remove empty lines\n",
    "    lines = [line for line in lines if line != '']\n",
    "    \n",
    "    columns = []\n",
    "    data = []\n",
    "    for index, line in enumerate(lines):\n",
    "        if line.startswith('@attribute'):\n",
    "            columns.append(line)\n",
    "            \n",
    "        if line.startswith('@data'):\n",
    "            # get the rest of the lines excluding the one that says @data\n",
    "            data = lines[index+1:]\n",
    "            break\n",
    "            \n",
    "    # clean column names -- '@attribute colname  \\t\\t\\t{a, b, ...}'\n",
    "    cleaned_columns = [c[11:c.index('real')].strip() for c in columns[:-1]]\n",
    "    \n",
    "    # ** change for real values. skip last column and parse differently\n",
    "    class_val = columns[-1]\n",
    "    cleaned_columns.append(class_val[11:class_val.index('{')].strip())\n",
    "    \n",
    "    # clean and split data\n",
    "    cleaned_data = [d.replace(', ', ',').split(',') for d in data]\n",
    "    \n",
    "    # create dataframe\n",
    "    return pd.DataFrame(cleaned_data, columns = cleaned_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    ys = df.iloc[:,-1]\n",
    "    ys = ys.values\n",
    "    \n",
    "    # change xs to 2d numpy array -- convert strings to floats\n",
    "    xs = df.iloc[:,:-1].astype(float)\n",
    "    xs = xs.values\n",
    "    \n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    \"\"\" Calculates the euclidean distance between two points \"\"\"\n",
    "    assert np.size(x1) == np.size(x2)\n",
    "\n",
    "    # Squared distance between each coordinate\n",
    "    distances = np.square(x1 - x2)\n",
    "    return np.sqrt(sum(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_centroids(X, k):\n",
    "    \"\"\"\n",
    "    Returns a matrix representing k randomly chosen instances for the initial centroids\n",
    "    \"\"\"\n",
    "    n_instances, n_features = np.shape(X)\n",
    "    centroids = np.zeros((k, n_features))\n",
    "    \n",
    "    # use random.sample to avoid picking the same instance\n",
    "    random_indices = random.sample(range(n_instances), k)\n",
    "    \n",
    "    for i, instance in enumerate(random_indices):\n",
    "        centroids[i] = X[instance]\n",
    "        \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_centroid(instance, centroids):\n",
    "    \"\"\"\n",
    "    Helper method for create_clusters.\n",
    "    \n",
    "    Returns the index of the closest centroid for a given instance\n",
    "    Distance measured using euclidean_distance\n",
    "    \"\"\"\n",
    "    closest = -1\n",
    "    closest_dist = float('inf')\n",
    "    for i, c in enumerate(centroids):\n",
    "        dist = euclidean_distance(instance, c)\n",
    "        if dist < closest_dist:\n",
    "            closest_dist = dist\n",
    "            closest = i\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(X, k, centroids):\n",
    "    \"\"\"\n",
    "    Returns a list of k-lists, each containing the indices of instances that are closest to the centroid\n",
    "    \n",
    "    ** stop storing the instances themselves it wastes space, just save indices\n",
    "    \"\"\"\n",
    "    n_instances, n_features = np.shape(X)\n",
    "    clusters = [[] for _ in range(k)]     # create clusters of centroids\n",
    "    \n",
    "    for i, x_i in enumerate(X):\n",
    "        centroid_idx = find_nearest_centroid(x_i, centroids)\n",
    "        clusters[centroid_idx].append(i)\n",
    "    \n",
    "    assert sum([len(c) for c in clusters]) == n_instances # sanity check\n",
    "    \n",
    "    return clusters\n",
    "    # turn list of np.arrays into list of 2D arrays\n",
    "#     return [np.reshape(c, newshape = (len(c), n_features)) for c in clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each cluster 1...k, calculate new centroid = mean of all points assigned to that cluster\n",
    "def update_centroids(X, k, clusters):\n",
    "    n_features = np.shape(X)[1]\n",
    "    new_centroids = np.zeros((k, n_features))\n",
    "    \n",
    "    for i, clstr in enumerate(clusters):\n",
    "        centroid = np.mean(X[clstr], axis=0)\n",
    "        new_centroids[i] = centroid\n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_groups(X, clusters):\n",
    "    \"\"\"\n",
    "    Return a vector of len n_instances that correspond to 0, 1, ... k \n",
    "    that corresponds to the cluster each instance was in at the end of training\n",
    "    \"\"\"\n",
    "    preds = np.zeros(np.shape(X)[0])\n",
    "    for i, c in enumerate(clusters):\n",
    "        for instance in c:\n",
    "            preds[instance] = i\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predition_map(X, k, clusters):\n",
    "    \"\"\"\n",
    "    *Note:* uses true y value -- put in seperate function of generating predictions because of this\n",
    "    This function returns a dict from our cluster numbers 0, 1, ... k -> actual class values\n",
    "    \"\"\"\n",
    "    # map clusters to classes\n",
    "    result = {k : 0 for k in range(k)}\n",
    "    for i, c in enumerate(clusters):\n",
    "        class_map = {class_val : 0 for class_val in y} # count number of each class per cluster to find most popular\n",
    "        \n",
    "        for instance in c:\n",
    "            val = y[instance]\n",
    "            class_map[val] = class_map.get(val, 0) + 1 # update counts\n",
    "            \n",
    "        most_popular_class = max(class_map, key=class_map.get)\n",
    "        result[i] = most_popular_class\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "from collections import Counter\n",
    "def predict(X):\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        clusters = create_clusters(X, k, centroids)\n",
    "        \n",
    "        prev_centroids = centroids\n",
    "        \n",
    "        centroids = update_centroids(X, k, clusters)\n",
    "       \n",
    "        # If no centroids have changed => convergence\n",
    "        delta = centroids - prev_centroids\n",
    "        if np.all((delta == 0)):\n",
    "            break\n",
    "    preds = generate_predictions(X, clusters)\n",
    "    print(Counter(preds))\n",
    "    pred_map = generate_predition_map(X, k, clusters)\n",
    "    results = [pred_map[p] for p in preds]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kMeans():\n",
    "    \n",
    "    def __init__(self, k, n_iter=100):\n",
    "        self.k = k\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def _initialize_centroids(self, X):\n",
    "        \"\"\"\n",
    "        Returns a matrix representing k randomly chosen instances for the initial centroids\n",
    "        \"\"\"\n",
    "        n_instances, n_features = np.shape(X)\n",
    "        centroids = np.zeros((self.k, n_features))\n",
    "\n",
    "        # use random.sample to avoid picking the same instance\n",
    "        random_indices = random.sample(range(n_instances), self.k)\n",
    "\n",
    "        for i, instance in enumerate(random_indices):\n",
    "            centroids[i] = X[instance]\n",
    "\n",
    "        return centroids\n",
    "    \n",
    "    def _find_nearest_centroid(self, instance, centroids):\n",
    "        \"\"\"\n",
    "        Helper method for create_clusters.\n",
    "\n",
    "        Returns the index of the closest centroid for a given instance\n",
    "        Distance measured using euclidean_distance\n",
    "        \"\"\"\n",
    "        closest = -1\n",
    "        closest_dist = float('inf')\n",
    "        for i, c in enumerate(centroids):\n",
    "            dist = euclidean_distance(instance, c)\n",
    "            if dist < closest_dist:\n",
    "                closest_dist = dist\n",
    "                closest = i\n",
    "        return closest\n",
    "    \n",
    "    def _create_clusters(self, X, centroids):\n",
    "        \"\"\"\n",
    "        Returns a list of k-lists, each containing the indices of instances that are closest to the centroid\n",
    "\n",
    "        ** stop storing the instances themselves it wastes space, just save indices\n",
    "        \"\"\"\n",
    "        n_instances, n_features = np.shape(X)\n",
    "        clusters = [[] for _ in range(self.k)]  # create clusters as empty lists and append indices to it\n",
    "\n",
    "        for i, x_i in enumerate(X):\n",
    "            centroid_idx = self._find_nearest_centroid(x_i, centroids)\n",
    "            clusters[centroid_idx].append(i)\n",
    "\n",
    "        assert sum([len(c) for c in clusters]) == n_instances # sanity check\n",
    "\n",
    "        return clusters\n",
    "    \n",
    "    def _update_centroids(self, X, clusters):\n",
    "        \"\"\"\n",
    "        Calculates a new centroid/centers by computing the mean of each cluster (by attribute value)\n",
    "        Returns matrix of k updated centroids that are each len(n_features)\n",
    "        \"\"\"\n",
    "    \n",
    "        n_features = np.shape(X)[1]\n",
    "        new_centroids = np.zeros((self.k, n_features))\n",
    "\n",
    "        for i, clstr in enumerate(clusters):\n",
    "            centroid = np.mean(X[clstr], axis=0) # select all rows with indices in cluster, axis=0 --> cols\n",
    "            new_centroids[i] = centroid\n",
    "        return new_centroids\n",
    "    \n",
    "    def train(self, X):\n",
    "        centroids = self._initialize_centroids(X) # pick k random centroids to start\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            clusters = self._create_clusters(X, centroids) # generate clusters from centroids\n",
    "\n",
    "            prev_centroids = centroids # hold onto previous\n",
    "\n",
    "            centroids = self._update_centroids(X, clusters) # update new centroids with current\n",
    "\n",
    "            # If no centroids have changed => convergence\n",
    "            delta = centroids - prev_centroids\n",
    "            if np.all((delta == 0)):\n",
    "                break\n",
    "\n",
    "        self.final_clusters = clusters\n",
    "        \n",
    "    \n",
    "    def _generate_prediction_groups(self, X):\n",
    "        \"\"\"\n",
    "        Return a vector of len n_instances that correspond to 0, 1, ... k \n",
    "        that corresponds to the cluster each instance was in at the end of training\n",
    "        \"\"\"\n",
    "        preds = np.zeros(np.shape(X)[0])\n",
    "        for i, c in enumerate(self.final_clusters):\n",
    "            for instance_index in c:\n",
    "                preds[instance_index] = i\n",
    "        return preds\n",
    "    \n",
    "    def _generate_predition_map(self, X, y):\n",
    "        \"\"\"\n",
    "        *Note:* uses true y value -- keep in seperate function to avoid issues\n",
    "        This function returns a dict from our cluster numbers 0, 1, ... k -> actual class values\n",
    "        \"\"\"\n",
    "        result = {k : 0 for k in range(k)}         # map clusters to classes\n",
    "        for i, c in enumerate(self.final_clusters):\n",
    "            \n",
    "            # count number of each class per cluster to find most popular\n",
    "            class_map = {class_val : 0 for class_val in y}  # maps class_val -> count\n",
    "            for instance in c:\n",
    "                val = y[instance] # look up actual val from y\n",
    "                class_map[val] = class_map.get(val, 0) + 1 # update counts\n",
    "\n",
    "            most_popular_class = max(class_map, key=class_map.get)\n",
    "            result[i] = most_popular_class\n",
    "        return result\n",
    "    \n",
    "    def predict(self, X, y):\n",
    "        preds = self._generate_prediction_groups(X)\n",
    "        pred_map = self._generate_predition_map(X, y)\n",
    "        results = [pred_map[p] for p in preds] # apply map to change cluster-index 0...k to actual class_vals\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess_data(readArff(\"iris\"))\n",
    "k = len(set(y))\n",
    "km = kMeans(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.train(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = km.predict(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11333333333333329"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - accuracy_score(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
